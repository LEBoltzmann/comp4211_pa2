{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEBoltzmann/comp4211_pa2/blob/master/autoregressive_model_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clone data\n",
        "!git clone https://github.com/LEBoltzmann/comp4211_pa2.git\n",
        "!pip install -r comp4211_pa2/pa2/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi5pBn8GgMzD",
        "outputId": "15cf37c8-e791-434d-8b93-24993da408be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'comp4211_pa2'...\n",
            "remote: Enumerating objects: 52506, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 52506 (delta 1), reused 3 (delta 0), pack-reused 52498\u001b[K\n",
            "Receiving objects: 100% (52506/52506), 96.61 MiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (231/231), done.\n",
            "Updating files: 100% (52503/52503), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r comp4211_pa2/pa2/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r comp4211_pa2/pa2/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons->-r comp4211_pa2/pa2/requirements.txt (line 1)) (23.0)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->-r comp4211_pa2/pa2/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons->-r comp4211_pa2/pa2/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons->-r comp4211_pa2/pa2/requirements.txt (line 1)) (6.1.0)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0 typeguard-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7V9CkxqcdcA"
      },
      "source": [
        "# AutoregressiveModel\n",
        "\n",
        "**Description:** AutoregressiveModel implemented in Keras to generate image.\n",
        "\n",
        "**Objective:** The objective of this assignment is to practise using the TensorFlow machine learning framework\n",
        "through implementing custom training modules and data reader modules for image generation on\n",
        "the Chinese Calligraphy dataset using a convolutional neural network (CNN) based architecture.\n",
        "Throughout the assignment, students will be guided to develop the CNN-based model step by\n",
        "step and study how to build custom modules on TensorFlow and the effects of different model\n",
        "configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5qADsjVcdcG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Image generation is one of the fundamental computer vision tasks, referring to the process of generating new images that are visually realistic and similar to real-world images. It is widely used in many applications, such as super resolution, photograph editing and 3D modelling. \n",
        "\n",
        "One approach to image generation is to use models that learn to predict the probability distribution of pixel values, given the values of all the previous pixels. These models generate images one pixel at a time, using the previously generated pixels to condition the generation of the next pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6pmZLclieHE"
      },
      "source": [
        "### Setting environment\n",
        "\n",
        "Note: You can only use the packages listed below !!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nzuClBlOcdcI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#enable numpy in tensor\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2uXmIzcdcK"
      },
      "source": [
        "## Getting the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-ajC_RQxam"
      },
      "source": [
        "### Download dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPs8yqGRNWix"
      },
      "outputs": [],
      "source": [
        "# Download dataset from google drive\n",
        "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n",
        "! mkdir ./data && unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n",
        "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n",
        "! unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n",
        "! ls -l ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrivlNhIRIgp"
      },
      "source": [
        "### make dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EWlGYkLdRGYo"
      },
      "outputs": [],
      "source": [
        "# Model / data parameters\n",
        "input_shape = (32, 32, 1)\n",
        "batch_size = 32\n",
        "data_dir = \"comp4211_pa2/pa2\"\n",
        "data_name = \"calligraphy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "adlqUIs5VXZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "e0dd012d-b8ef-4890-b8e3-de63cdb3c28c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-823357bdf981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# final shape should be 1313 (32, 32, 32, 1) (32, 32, 32, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalligraphySequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-823357bdf981>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m### [C2: Build getitem function]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CalligraphySequence' object has no attribute 'x'"
          ]
        }
      ],
      "source": [
        "# dataset class\n",
        "class CalligraphySequence(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, image_dir, batch_size):\n",
        "        ### [C1: Build init and len functions]\n",
        "        # Your code here\n",
        "        self.batch_size = batch_size\n",
        "        self.image_dir = image_dir + \"/train\"\n",
        "        self.files = os.listdir(self.image_dir)\n",
        "        self.file_num = len(self.files)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        ### [C1: Build init and len functions]\n",
        "        # Your code here\n",
        "        \n",
        "        return math.ceil(self.file_num / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #get index \n",
        "        low = idx * self.batch_size\n",
        "        high = min(low + self.batch_size, self.file_num)\n",
        "        batch_x = self.x[low:high]\n",
        "        batch_y = self.y[low:high]\n",
        "        ### [C2: Build getitem function]\n",
        "        # Round all pixel values less than 33% of the max 256 value to 0\n",
        "        # anything above this value gets rounded up to 1 so that all values are either\n",
        "        # 0 or 1\n",
        "        # Your code here\n",
        "        return (batch_x, batch_y)\n",
        "\n",
        "# final shape should be 1313 (32, 32, 32, 1) (32, 32, 32, 1)\n",
        "train_ds = CalligraphySequence(data_dir, batch_size)\n",
        "print(len(train_ds), train_ds[0][0].shape, train_ds[0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to gray\n",
        "image_dir = data_dir + \"/train\"\n",
        "files = os.listdir(image_dir)\n",
        "os.system('mkdir ' + image_dir + '/grey')\n",
        "\n",
        "for image in files:\n",
        "    img = Image.open(image_dir + '/' + image).convert('L')\n",
        "    img.save(image_dir + '/grey/' + image)"
      ],
      "metadata": {
        "id": "qTS8T5icQe0R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVekI5LcdcL"
      },
      "source": [
        "## Create layers for the requisite Layers for the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK-wOq1WkaAv"
      },
      "source": [
        "### Given function for conv2d / down_shift / right_shift / concat_elu\n",
        "1. conv2d: 2d convolution layer using layers.Conv2D\n",
        "\n",
        "2. down_shift: shift feature down in height dimension (by padding zero to the top and drop the bottom)\n",
        "\n",
        "3. right_shift: shift feature right in width dimension\n",
        "\n",
        "4. concat_elu: a nonlinearity layer (http://arxiv.org/abs/1603.05201)\n",
        "\n",
        "The down_shift and right_shift functions are used to avoid information leaks in a causal network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDUbrK8XkZr6"
      },
      "outputs": [],
      "source": [
        "class Conv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[3, 3], stride=[1, 1], pad='SAME', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = layers.Conv2D(num_filters, filter_size, padding = pad, strides = stride, activation = nonlinearity, \n",
        "                         kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "def down_move(x, step=1):\n",
        "    input_shape = tf.shape(x)\n",
        "    return tf.concat([tf.zeros((input_shape[0], step, input_shape[2], input_shape[3])), x[:, :input_shape[1] - step, :, :]], 1)\n",
        "\n",
        "def right_move(x, step=1):\n",
        "    input_shape = tf.shape(x)\n",
        "    return tf.concat([tf.zeros((input_shape[0], input_shape[1], step, input_shape[3])), x[:, :, :input_shape[2] - step, :]], 2)\n",
        "\n",
        "def concat_elu(x):\n",
        "    \"\"\" like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU \"\"\"\n",
        "    axis = len(x.get_shape()) - 1\n",
        "    out = tf.nn.elu(tf.concat([x, -x], axis))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgKeYdBploK-"
      },
      "source": [
        "### Gated Residual Block\n",
        "The GatedResnet class applies gated residual connections to input tensors for feature extraction.\n",
        "\n",
        "Please follow Section 4.2.3 to implement coding question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy9VWOO0mDHs"
      },
      "outputs": [],
      "source": [
        "class DownMovedConv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[2, 3], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C4: Build DownMovedConv2d.]\n",
        "        # Your code here\n",
        "        \n",
        "\n",
        "    def call(self, x):\n",
        "        ### [C4: Build DownMovedConv2d.]\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "class DownRightMovedConv2d(layers.Layer):\n",
        "    def __init__(self, num_filters, filter_size=[2, 2], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C3: Build DownRightMovedConv2d.]\n",
        "        # Your code here\n",
        "\n",
        "    def call(self, x):\n",
        "        ### [C3: Build DownRightMovedConv2d.]\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "class TensorDense(layers.Layer):\n",
        "    def __init__(self, num_units, nonlinearity=None, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C5: Build TensorDense.]\n",
        "        # Your code here\n",
        "\n",
        "    def call(self, x):\n",
        "        ### [C5: Build TensorDense.]\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "class GatedResnet(layers.Layer):\n",
        "    def __init__(self, num_filters, nonlinearity=concat_elu, **kwargs):\n",
        "        super().__init__()\n",
        "        ### [C6: Build GatedResnet.]\n",
        "        # Your code here\n",
        "\n",
        "    def call(self, x):\n",
        "        ### [C6: Build GatedResnet.]\n",
        "        # Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFo2cKKoKtv"
      },
      "source": [
        "### Main AutoregressiveModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8RHOpB_mHQP"
      },
      "outputs": [],
      "source": [
        "class AutoregressiveModel(layers.Layer):\n",
        "    def __init__(self, n_resnet=5, n_filters=256, n_block=12, n_output=10, **kwargs):\n",
        "        super().__init__()\n",
        "        self.n_resnet = n_resnet\n",
        "        self.n_filters = n_filters\n",
        "        self.n_block = n_block\n",
        "        self.n_output = n_output\n",
        "        # init all network layers\n",
        "        self.down_moved_conv2d = DownMovedConv2d(num_filters=self.n_filters, filter_size=[1, 3])\n",
        "        self.down_right_moved_conv2d = DownRightMovedConv2d(num_filters=self.n_filters, filter_size=[2, 1])\n",
        "        self.ul_list_gated_resnet = []\n",
        "        self.ul_list_dense_layer = []\n",
        "        ### [C7: Build AutoregressiveModel.]\n",
        "        # Your code here\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        x = down_move(self.down_moved_conv2d(inputs)) + right_move(self.down_right_moved_conv2d(inputs))\n",
        "        ### [C7: Build AutoregressiveModel.]\n",
        "        # Your code here\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEF1CtrPcdcO"
      },
      "source": [
        "## Build the model based on the original paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZzvQW8jcdcP"
      },
      "outputs": [],
      "source": [
        "## Build the model based on the original paper\n",
        "inputs = keras.Input(shape=input_shape, dtype=tf.float32)\n",
        "x = AutoregressiveModel(n_resnet=6, n_filters=64, n_block=6, n_output=10)(inputs)\n",
        "out = keras.layers.Conv2D(\n",
        "    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",
        ")(x)\n",
        "\n",
        "pixel_cnn = keras.Model(inputs, out)\n",
        "\n",
        "### [C11: Model training and log reporting]\n",
        "# you can use keras.optimizers.Adam here to define \"adam\"\n",
        "# compile your model and make a summary on its architecture\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### [C8: Load the pretrained weights]\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "3ruvOoJtQyDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOocGHbkPlsl"
      },
      "outputs": [],
      "source": [
        "### [C11: Model training and log reporting]\n",
        "# you can use model.fit here\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH4M5H4KOh31"
      },
      "outputs": [],
      "source": [
        "# save weights \n",
        "# pixel_cnn.save_weights('pixel_cnn_e15.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlv-hdhcdcQ"
      },
      "source": [
        "## Demonstration\n",
        "\n",
        "The AutoregressiveModel cannot generate the full image at once. Instead, it must generate each pixel in\n",
        "order, append the last generated pixel to the current image, and feed the image back into the\n",
        "model to repeat the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBULc1N8cdcS"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Create an empty array of pixels.\n",
        "batch = 4 # you may want to change this parameter \n",
        "pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n",
        "batch, rows, cols, channels = pixels.shape\n",
        "\n",
        "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
        "for row in tqdm(range(rows)):\n",
        "    for col in range(cols):\n",
        "        for channel in range(channels):\n",
        "            ### [C9: Qualitative Evaluation]\n",
        "            # Your code here\n",
        "            # 1. Feed the whole array and retrieving the pixel value probabilities for the next\n",
        "            # pixel. You can use model.predict function to get predict value for each pixel.\n",
        "\n",
        "            # 2. Use the probabilities to pick pixel values and append the values to the image\n",
        "            # frame. you can use tf.math.ceil to achieve the 0.5 threshold.\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Stack the single channeled black and white image to RGB values.\n",
        "    x = np.stack((x, x, x), 2)\n",
        "    # Undo preprocessing\n",
        "    x *= 255.0\n",
        "    # Convert to uint8 and clip to the valid range [0, 255]\n",
        "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "    return x\n",
        "\n",
        "\n",
        "# Iterate over the generated images and plot them with matplotlib.\n",
        "for i, pic in enumerate(pixels):\n",
        "    keras.preprocessing.image.save_img(\n",
        "        \"generated_image_{}.png\".format(i), deprocess_image(np.squeeze(pic, -1))\n",
        "    )\n",
        "\n",
        "display(Image(\"generated_image_0.png\"))\n",
        "display(Image(\"generated_image_1.png\"))\n",
        "display(Image(\"generated_image_2.png\"))\n",
        "display(Image(\"generated_image_3.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative Evaluation\n",
        "\n"
      ],
      "metadata": {
        "id": "VLCjV00RT78w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### [C10: Quantitative Evaluation]\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "yG8oF4E_UPIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}